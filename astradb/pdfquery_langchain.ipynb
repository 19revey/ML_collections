{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import find_dotenv,load_dotenv\n",
    "\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "# PDF reader\n",
    "from pypdf import PdfReader\n",
    "# Support for dataset retrieval with Hugging Face\n",
    "from datasets import load_dataset\n",
    "# Astra DB integration\n",
    "import cassio\n",
    "\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "ASTRA_DB_ID=os.getenv(\"ASTRA_DB_ID\")\n",
    "ASTRA_DB_APPLICATION_TOKEN=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "\n",
    "llm=GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.0,api_key=GOOGLE_API_KEY)\n",
    "embedding = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# llm=OpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0.5,openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "# llm.invoke(\"capital of china\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import glob\n",
    "\n",
    "# pdfreader=PdfReader('segregationforce.pdf')\n",
    "# raw_text=''\n",
    "# for page in pdfreader.pages:\n",
    "#     content=page.extract_text()\n",
    "#     if content:\n",
    "#         raw_text+=content\n",
    "\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "# )\n",
    "# chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "class readpdf:\n",
    "    def __init__(self, pdf_docs) :\n",
    "        self.pdf_docs=pdf_docs\n",
    "        self.text = \"\"\n",
    "\n",
    "    def get_pdf_text(self):\n",
    "        for pdf in self.pdf_docs:\n",
    "            pdf_reader = PdfReader(pdf)\n",
    "            for page in pdf_reader.pages:\n",
    "                content=page.extract_text() \n",
    "                if content:\n",
    "                    self.text += content \n",
    "\n",
    "    def get_text_chunks(self):\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len\n",
    "        )\n",
    "        chunks = text_splitter.split_text(self.text)\n",
    "        return chunks\n",
    "    \n",
    "pdf_files = glob.glob(\"./pdfs/*.pdf\")\n",
    "text = readpdf(pdf_files)\n",
    "text.get_pdf_text()\n",
    "chunks=text.get_text_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_texts(texts=chunks, embedding=embedding)\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector store in astradb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN,database_id=ASTRA_DB_ID)\n",
    "astra_vector_store=Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name=\"demo\",\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    ")\n",
    "\n",
    "astra_vector_store.add_texts(chunks)\n",
    "print(\"Inserted %i headlines.\" % len(chunks))\n",
    "astra_vector_index=VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a researcher know how to write scientific paper on topics related to STEM. \n",
    "I will share related texts from previous documents with you and you will rewrite the provided texts in an academic language.\n",
    "\n",
    "1/ the generated texts should be very similar to the style of the documents, \n",
    "in terms of ton of voice, logical arguments and other details\n",
    "\n",
    "2/ If the past texts are irrelevant, then try to mimic the style of the documents to rewrite the paragraph\n",
    "\n",
    "Below is the texts I want to rewrite:\n",
    "{paragraph}\n",
    "\n",
    "Here is a list of previous documents:\n",
    "{document}\n",
    "\n",
    "Please rewrite the paragraph:\n",
    "    \"\"\"\n",
    "prompt=PromptTemplate(template=prompt_template,input_variables=[\"paragraph\",\"document\"])\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "query_text=\"particle size segregation and diffusive remixing\"\n",
    "# docs=astra_vector_store.similarity_search(query_text,k=4)\n",
    "\n",
    "new_db = FAISS.load_local(\"faiss_index\", embedding)\n",
    "docs = new_db.similarity_search(query_text,k=5)\n",
    "response=chain.run(paragraph=query_text,document=docs)\n",
    "# answer=astra_vector_index.query(query_text,llm=llm).strip()\n",
    "print(response)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run QA cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question=True\n",
    "while True:\n",
    "    if first_question:\n",
    "        query_text=input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
    "    else:\n",
    "        query_text=input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()   \n",
    "    if query_text.lower()=='quit':\n",
    "        break\n",
    "    if query_text==\"\":\n",
    "        continue\n",
    "\n",
    "    first_question=False \n",
    "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
    "\n",
    "    answer=astra_vector_index.query(query_text,llm=llm).strip()\n",
    "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
    "\n",
    "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
    "    for doc,score in astra_vector_store.similarity_search_with_score(query_text,k=4):\n",
    "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
